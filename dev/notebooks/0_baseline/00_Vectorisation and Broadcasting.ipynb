{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a function f\n",
    "Machine learning algorithms aim to **learn a target function $f$** that best maps input variables ($X$) to an output variable ($Y$):\n",
    "\n",
    "\n",
    "$$ Y = f(X) $$\n",
    "\n",
    "What function $f$ *exactly* is or *how* it looks, is not known. And exactly that (what the function $f$ has to be) is the part that Machine Learning tries to learn from the data.\n",
    "\n",
    "\n",
    "## Making predictions\n",
    "\n",
    "The main goal of machine learning is to learn the underlying function in order to **make predictions** of $Y$ for new $X$ (unseen data). When we *train* or *learn* a machine learning model we are *estimating* the target function (the target function is the best mapping from $Y$ given $X$ from the data that is available). Usually a lot of time is spent trying to improve this estimate of the unknown target function in order to improve performance of the predictions made by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy data\n",
    "Let us make some dummy data $X$. This is a matrix that contains $n$ observations, each observations having $k$ features, thus forming an $n \\times k$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "k = 3\n",
    "X = np.random.randint(10, size=(n, k))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the first observation consists of three features. The first feature has a value of 6, the second of 3, the third of 7. To make predicication, we will want to learn what weight we should assign to every feature. Not every feature will be as important in predicting an outcome. This will depend on what we want to predict.\n",
    "\n",
    "However, at the beginning of each learning algorithm we will randomize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "W = np.random.rand(k, 1)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal will be to learn the proper weights for our type of prediction.\n",
    "\n",
    "E.g. If the features are length, weight and body temperature, the first two features will be roughly equally important in predicting the size of someones clothes, while the third will be of no importance. \n",
    "\n",
    "The final weights for this case might look something like $[0.9, 0.8, 0.01]$\n",
    "\n",
    "However, for predicting shoe size, the first feature will be more important than the last one. We might get something like $[0.9, 0.3, 0.01]$. \n",
    "\n",
    "For predicting infections, the last feature will be the most imporant, e.g. $[0.01, 0.1, 0.9]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forloop\n",
    "If we want to make a prediction, often denoted as $\\hat{y}$ (y-hat), a very general way to do so is to multiply every feature with a weight:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\hat{y} = w_1 x_1 + \\dots + w_n x_n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This approach is a basic element in many machine learning algorithms. We might add some extra tricks, like Baysian Statistics, SVM's or stacking Neural Network layers upon eachother.\n",
    "\n",
    "A naive way to calculate this, would be while using a forloop. Making an empty matrix of zeros with the right shape is much faster then appending an outcome to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop(X, W):\n",
    "    result = np.zeros((X.shape[0],W.shape[1]))\n",
    "    # for every observation\n",
    "    for i, observation in enumerate(X):\n",
    "        yhat = 0\n",
    "        # we want to multiply every feature with a weight\n",
    "        for j, feature in enumerate(observation):\n",
    "            # and make a summation of every weigth * feature\n",
    "            yhat += W[j] * feature\n",
    "        result[i, 0] = yhat\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can time a piece of code like this. This will run the line enough times to obtain a good average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "looptime = %timeit -o for_loop(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's scale that up and do a gridsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "np.random.seed(42)\n",
    "nrange = range(2,5)\n",
    "krange = range(1,3)\n",
    "heatmap_for = np.zeros((len(nrange),len(krange)))\n",
    "\n",
    "for i, n in enumerate([10**i for i in nrange]):\n",
    "    for j, k in enumerate([10**i for i in krange]):\n",
    "        X = np.random.randint(10, size=(n, k))\n",
    "        W = np.random.rand(k, 1)\n",
    "        looptime = %timeit -o for_loop(X, W)\n",
    "        heatmap_for[i, j] = looptime.average\n",
    "        logger.info(f\"n={n}, k={k}, avg={looptime.average:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_for, annot=heatmap_for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, I get results that:\n",
    "- scale linear: multiplying the amount of observations or features by 10, will add about a factor of 10 to the time.\n",
    "- ranges between 0.0029 sec (for 100 observations with one feature) and 2.7 sec (for 10.000 observations and 100 features) on average.\n",
    "\n",
    "## vectorisation\n",
    "Now, we will vectorize this calculation.\n",
    "\n",
    "Note that, for matrix multiplication to work, we will need the dimensions of the two matrices to match. If our first matrix $X$ has dimensions $(n, k)$ and our second matrix $W$ has dimensions $(k, m)$, we are able to do $X \\dot W$ because the dimension $k$ is equal.\n",
    "\n",
    "So, trying to do a matrix multiplication with dimensions (10, 3) and (3, 1) will work, but (10,3) and (4,1) will fail because $3\\neq 4$.\n",
    "\n",
    "Also, trying to multiply (3,1) and (10, 3) will fail because $1 \\neq 10$. This means the order is important. This is different from normal multiplication, where $2 \\times 3$ and $3 \\times 2$ both equal to 6.\n",
    "\n",
    "Let's say our data has $n=2$ cases ($a$ and $b$), where we observe $k=2$ features for every case. We now need two weights $w_1$ and $w_2$, one for every feature. A matrix multiplication would do exactly what we did with the forloop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{1} & a_2 \\\\\n",
    "b_{1} & b_{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_1*w_1 + a_2*w_2\\\\\n",
    "b_1*w_1 + b_2*w_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we implement the same formula, we see that everything gets much more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation(X, W):\n",
    "    return np.dot(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the same experiment, but just swap the forloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nrange = range(2,5)\n",
    "krange = range(1,3)\n",
    "heatmap_vec = np.zeros((len(nrange),len(krange)))\n",
    "\n",
    "for i, n in enumerate([10**i for i in nrange]):\n",
    "    for j, k in enumerate([10**i for i in krange]):\n",
    "        X = np.random.randint(10, size=(n, k))\n",
    "        W = np.random.rand(k, 1)\n",
    "        looptime = %timeit -o vectorisation(X, W)\n",
    "        heatmap_vec[i, j] = looptime.average\n",
    "        logger.info(f\"n={n}, k={k}, avg={looptime.average:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_vec, annot=heatmap_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is impressive. Not only is the linear scaling gone, but this is really an amazing amount faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = heatmap_for / heatmap_vec\n",
    "sns.heatmap(speedup, annot=speedup,fmt='.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that is right. For the very small case (100 observations, 1 feature) the speedup is about 1400 times.\n",
    "\n",
    "For larger scales, if you are using newer versions of python (eg 3.11) the speedup isnt as impressive with older versions (used to be a factor 3000, even 4000).\n",
    "However, being 300 times slower than someone else is still not something to be proud of.\n",
    "\n",
    "I hope you can imagine how extremely useful this can be. Even when python has gotten 10x faster in some cases, a factor 300x can still make the difference between useful and useless.\n",
    "\n",
    "The same can be done with other functions, like calculating a sine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(100)\n",
    "%timeit for x in X: np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, instead of the forloop, we can do that in way that is both simpler and faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us the exact same results.\n",
    "\n",
    "In addition to this, we don't have to worry too much about adding extra dimensions. Numpy simply scales things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100,100)\n",
    "x1 = np.log(X)\n",
    "x2 = np.exp(X)\n",
    "X = np.random.rand(10, 10, 10)\n",
    "x3 = np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation on matrix multiplication is element wise multiplication, also known as the Hadamard product. Compare the two examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],\n",
    "              [3,4]])\n",
    "\n",
    "B = np.array([[10,20],\n",
    "              [30,40]])\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate $(1 \\times 10) + (2 \\times 30)$ for the first entry, which is 70.\n",
    "\n",
    "However, what if we wanted to multiply every element with the same element in the other matrix (so, every element at position $(i,j)$ in $A$ is multiplied with every $(i,j)$ element in $B$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equals to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting referes to how numpy handles arrays with different shapes. In general, the smaller array is \"broadcast\" across the larger array to get compatible shapes. Multiplying 1-dimensional array of the same length performs a element-wise multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the dimensions don't match, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "try:\n",
    "     a * b\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this constraint is relaxed when the dimensions meet certain constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = 2.0\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([2.0, 2.0, 2.0])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just shorter. In the first case, we can think of `b` being strethced to an array of the same shape as `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, two dimensions are compatible when:\n",
    "1. they are equal\n",
    "2. one of them is 1\n",
    "\n",
    "In addition to this, array's do not need to have the same number of dimensions. Eg an image with dimensions of `256x256` pixels can have three colors. This results in a `256x256x3` shape matrix. If we want to scale each color, we can use a 1-dimensional array with 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.random.rand(256, 256, 3)\n",
    "scale = np.array([2, 3, 6])\n",
    "output = image * scale\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"broadcasting\" works along multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(15, 3, 5)\n",
    "B = np.random.rand(15, 1, 5)\n",
    "C = A * B\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(8, 1, 6, 1)\n",
    "B = np.random.rand(7, 1, 5)\n",
    "C = A*B\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out what happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "You receive data from 1000 patients. For every patient, there are 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "k = 8\n",
    "X = np.random.rand(n, k)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to build a simple linear model, and start by implementing the formula $f(x) = \\sum_{i=1}^n w_i * x_i $\n",
    "\n",
    "First, you initialize random weights. If you want to use matrix multiplication, what is the shape you need for the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you multiply $X$ with $W$, what do you expect the shape of the outcome to be? In the outcome, what is the meaning of every row? And every column? Implement the forumula by using `np.dot`. Did you get the dimensions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume that instead of predicting one outcome, you want to predict two different things (e.g., both the expected duration of a stay, and the chance of complications during the stay). Every patient still has 8 features, but the outcome should be two numbers, instead of one.\n",
    "\n",
    "How do you need to change the shape of $W$?\n",
    "\n",
    "What is the expected shape if you apply matrix multiplication between $X$ and $W$? What would happen if you multiply $W$ and $X$?\n",
    "\n",
    "Create $W$ and implement the new calculation. How does this differ from the calculation with just one outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c42a0a2900956ab7a89ede89f2c46c7e488ca7435c057b96fa71de27f73f979b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
