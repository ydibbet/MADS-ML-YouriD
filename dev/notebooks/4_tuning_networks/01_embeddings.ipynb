{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start with a simple corpus of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"the cat sat on the mat\", \"where is the cat\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models wont handle strings. So, what we typically do is to `tokenize` the words. We will assign arbitrary integers to the words.\n",
    "\n",
    "First, we need to get all the words. So we split the strings on spaces, which gives us the words in every sentence.\n",
    "\n",
    "Then, we flatten the sentences so we get one very long list of words. After that, we run through the corpus to count all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def split_and_flat(corpus: List[str]):\n",
    "    corpus = [x.split() for x in corpus]\n",
    "    corpus = [x for y in corpus for x in y]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "data = split_and_flat(corpus)\n",
    "counter = Counter(data)\n",
    "counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is usefull, if our corpus grows too big and we want to drop some words. We better drop the less frequent words in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dict = OrderedDict(counter)\n",
    "ordered_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can build a vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "v1 = vocab(ordered_dict)\n",
    "v1.set_default_index(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a mapping from tokens to arbitrary integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[\"the\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default index is returned when we have unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can translate back to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.lookup_token(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are now able to map the sentence from strings to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = corpus[0].split()\n",
    "tokenized_sentence = [v1[word] for word in sentence]\n",
    "tokenized_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you \"read\" the original sentence?\n",
    "\n",
    "Ok, now, how to represent this. A naive way would be to use a one hot encoding.\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/guide/images/one-hot.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tokenized_tensor = torch.tensor(tokenized_sentence)\n",
    "oh = F.one_hot(tokenized_tensor)\n",
    "oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this might seem like a nice workaround, it is very memory inefficient. \n",
    "Vocabularies can easily grow into the 10.000+ words!\n",
    "\n",
    "So, let's make a more dense space. We simply decide on a dimensionality, and start with assigning a random vector to every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.tensorflow.org/text/guide/images/embedding2.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(v1)\n",
    "hidden_dim = 4\n",
    "\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size, embedding_dim=hidden_dim, padding_idx=-2\n",
    ")\n",
    "x = embedding(tokenized_tensor)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "- we started with a sentence of strings.\n",
    "- we map the strings to arbitrary integers\n",
    "- the integers are used with an Embedding layer; this is nothing more than a lookup table where every word get's a random vector assigned\n",
    "\n",
    "We started with a 6-word sentence. But we ended with a (6, 4) matrix of numbers.\n",
    "\n",
    "So, let's say we have a batch of 32 sentences. We can now store this for example as a (32, 15, 6) matrix: batchsize 32, length of every sentence is 15 (use padding if the sentence is smaller), and every word in the sentence represented with 6 numbers.\n",
    "\n",
    "This is exactly the same as what we did before with timeseries! We have 3 dimensional tensors, (batch x sequence_length x dimensionality) that we can feed into an RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x[None, ...]\n",
    "rnn = torch.nn.GRU(input_size=hidden_dim, hidden_size=16, num_layers=1)\n",
    "\n",
    "out, hidden = rnn(x_)\n",
    "out.shape, hidden.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
