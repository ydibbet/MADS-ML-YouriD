{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-MachineLearning-course/blob/master/notebooks/4_tuning_networks/04_transfer_learning_with_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Check if the notebook is running on Google Colab\n",
    "colab = False\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    colab = True\n",
    "    # Running on Google Colab; install loguru\n",
    "    !pip install mads_datasets mltrainer loguru\n",
    "else:\n",
    "    # Not running on Google Colab; you might be on a local setup\n",
    "    print(\"Not running on Google Colab. Ensure dependencies are installed as needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the flowers dataset from the first lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "flowersfactory = DatasetFactoryProvider.create_factory(DatasetType.FLOWERS)\n",
    "streamers = flowersfactory.create_datastreamer(batchsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just about 3000 images. To get more out of our data, we will use a technique called 'data augmentation'. When an image if flipped, or cropped, we get a different image, preventing the model to overfit on the quirks of this small dataset. We will also normalize the images to the mean and standard deviation used when training resnet; this is not strictly necessary, but should make it a bit easier for the model to work with our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to crop the images, lets make our images during preprocessing a bit bigger, so we actually have something to crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowersfactory.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowersfactory.settings.img_size = (500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modification of the settings, we can create the dataset.\n",
    "We can see our images are actually 500x500 pixels now.\n",
    "\n",
    "The transformations are just a function; we can input the img and get a transformed image out. Let try that,\n",
    "and visualise the result:\n",
    "\n",
    "PS: if you dont have enough RAM on colab (eg 12GB), the cell below might crash your notebook (because it recreates the dataset); \n",
    "the first cell in this notebook has set the value of 'colab' to True if you are on colab, to avoid this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab:\n",
    "    datasets = flowersfactory.create_dataset()\n",
    "    traindataset = datasets[\"train\"]\n",
    "    img, lab = traindataset[0]\n",
    "    logger.info(f\"original shape: {img.shape}\")\n",
    "    # original shape: torch.Size([3, 500, 500])\n",
    "    transformed_img = data_transforms[\"train\"](img)\n",
    "    logger.info(f\"transformed shape: {transformed_img.shape}\")\n",
    "    # transformed shape: torch.Size([3, 224, 224])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img.numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(transformed_img.numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the BasePreprocessor, we will squeeze in the transformer. Lets make that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentPreprocessor():\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "    def __call__(self, batch: list[tuple]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        X, y = zip(*batch)\n",
    "        X = [self.transform(x) for x in X]\n",
    "        return torch.stack(X), torch.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an separate preprocessor for train and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainprocessor= AugmentPreprocessor(data_transforms[\"train\"])\n",
    "validprocessor = AugmentPreprocessor(data_transforms[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add that as the preprocessor for train and validation streamers. We do it like this because by default we can only provide a single preprocessor for both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "train.preprocessor = trainprocessor\n",
    "valid.preprocessor = validprocessor\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(trainstreamer)\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets visualise a random batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "img = X.permute(0, 2, 3, 1).numpy()\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "img = std * img + mean\n",
    "img = np.clip(img, 0, 1)\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10,10))\n",
    "axs = axs.ravel()\n",
    "for i in range(9):\n",
    "    axs[i].imshow(img[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building our own resnet, we will just download a pretrained version. This saves us many hours of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "resnet = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = resnet(X)\n",
    "yhat.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the resnet is trained for 1000 classes. We have just 5...\n",
    "\n",
    "We will swap the last layer and retrain the model.\n",
    "\n",
    "First, we freeze all pretrained layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you study the resnet implementation on [github](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L206) you can see that the last layer is named `.fc`, like this:\n",
    "\n",
    "```\n",
    " self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    " ```\n",
    "\n",
    "This is a Linear layer, mapping from 512 * block.expansion to num_classes.\n",
    "\n",
    "\n",
    "so we will swap that for our own. To do so we need to figure out how many features go into the .fc layer.\n",
    "We can retrieve the incoming amount of features for the current `.fc` with `.in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resnet.fc))\n",
    "in_features = resnet.fc.in_features\n",
    "in_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's swap that layer with a minimal network. Sometimes just a linear layer is enough, sometimes you want to add two layers and some dropout.\n",
    "Play around to see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, 5)\n",
    "    # nn.Linear(in_features, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128, 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = resnet(X)\n",
    "yhat.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a fully trained resnet, but we added two layers at the end that transforms everything into 5 classes.\n",
    "These layers are random, so we need to train them for some epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to train (about 4 min per epoch), you could scale down to amount of trainsteps to speed things up.\n",
    "\n",
    "You will start with a fairly high learning rate (0.01), and if the learning stops, after patience epochs the learning rate gets halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    logger.warning(\"This model will take 15-20 minutes on CPU. Consider using accelaration, eg with google colab (see button on top of the page)\")\n",
    "logger.info(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use SGD as optimizer and a stepLR as scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD\n",
    "scheduler = optim.lr_scheduler.StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=30,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs/flowers\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    "    optimizer_kwargs= {'lr': 0.1, 'weight_decay': 1e-05, 'momentum': 0.9},\n",
    "    scheduler_kwargs= {'step_size' : 10, 'gamma' : 0.1},\n",
    "    earlystop_kwargs= None,\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=resnet,\n",
    "    settings=settings,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this will be very slow without acceleration!\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
