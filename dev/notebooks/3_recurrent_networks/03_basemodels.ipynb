{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SDdSZZAwLO7p"
   },
   "source": [
    "# Timeseries with recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YYqJb6HjLO77"
   },
   "source": [
    "# 1.0 Sunspots\n",
    "## 1.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "sunspotfactory = DatasetFactoryProvider.create_factory(DatasetType.SUNSPOTS)\n",
    "sunspotfactory.download_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = sunspotfactory.filepath\n",
    "f.exists(), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = np.genfromtxt(f, usecols=(2,3))\n",
    "spots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OVmz9rjNLO78"
   },
   "source": [
    "This is data on sunspots, since 1749, in a pandas dataframe. Let us cast this to a `float32` Torch Tensor.\n",
    "\n",
    "We first need to get the numpy data out of the pandas, and the cast that to a float32 Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensordata = torch.from_numpy(spots[:, 1]).type(torch.float32)\n",
    "tensordata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "avZhVdkzLO78",
    "outputId": "733b6bdc-6f39-4715-8220-ba7dc3d94ca4"
   },
   "outputs": [],
   "source": [
    "plt.plot(tensordata);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_AN2YU0SLO79"
   },
   "source": [
    "## 1.2 Train-test split\n",
    "There seems to be some pattern. But also a lot of noise. We need to take this into account for our final model: it might be hard to get very high accuracy, because we dont have any additional features (could you think of possible relevant features?).\n",
    "\n",
    "So let's make a train-test split, and normalize on the trainset. Note that we split on past-future! (Why is this important?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "R6HDt5OwLO79",
    "outputId": "cdaba7ce-fe61-479b-fc38-ff85e00ab39e"
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "split = 2500\n",
    "train = tensordata[:split]\n",
    "\n",
    "norm = max(train)\n",
    "test = tensordata[split:]\n",
    "\n",
    "train = train / norm\n",
    "test = test/ norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would go wrong if you normalize on the max of the testset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce steps back to months, starting in 1749\n",
    "idx0 = torch.arange(0, len(train)) / 12 + 1749\n",
    "idx1 = torch.arange(0, len(test)) / 12 + idx0.max()\n",
    "fig, axs = plt.subplots(2, sharex=True)\n",
    "axs[0].plot(idx0, train)\n",
    "axs[1].plot(idx1,test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QBWX872lLO7-"
   },
   "source": [
    "## 1.3 Windowing\n",
    "\n",
    "Now, let's use our windowed datagenerator.\n",
    "\n",
    "Regarding the windowsize, we will have to ask ourselves: what is a reasonable time, we will need to look into the past to be able to predict the future? If we make it too long, we will get irrelevant data (eg, does the amount of sunspots a hundred years still impact significantly the sunspots in 2020? If you think so, you should increase your window by a lot!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import datatools\n",
    "\n",
    "window_size = sunspotfactory.settings.window_size\n",
    "print(f\"windowsize: {window_size}\")\n",
    "\n",
    "idx = datatools.window(train, window_size)\n",
    "trainset = train[idx]\n",
    "idx = datatools.window(test, window_size)\n",
    "testset = test[idx]\n",
    "trainset.shape, testset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is looking good. We squeezed out 2475 training examples out of this long, single timeseries.\n",
    "\n",
    "## 1.4 A custom dataset\n",
    "\n",
    "Let's make the data 3D, just as our timeseries model will need to have it. We just have one feature, so that is just a dimension of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = trainset[...,None]\n",
    "testset = testset[..., None]\n",
    "trainset.shape, testset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a Dataset in PyTorch. \n",
    "According to the [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)\n",
    "> A custom Dataset class must implement three functions: `__init__`, ` __len__`, and `__getitem__`\n",
    "\n",
    "\n",
    "The functions with the double underscores are called \"dunder\" function. \n",
    "The `__len__` function is the function that is called when you do `len(object)`, the `__getitem__` function is called when you do `object[idx]` and `idx` is passed as an argument.\n",
    "\n",
    "We can inherit the `Dataset` class, and if we implement a `__len__` and `__getitem__` function, we are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets.base import DatasetProtocol\n",
    "from typing import Tuple\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class SunspotDataset(DatasetProtocol):\n",
    "    def __init__(self, data: Tensor, horizon: int) -> None:\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        # get a single item\n",
    "        item = self.data[idx]\n",
    "        # slice off the horizon\n",
    "        x = item[:-self.horizon,:]\n",
    "        y = item[-self.horizon:,:].squeeze(-1) # squeeze will remove the last dimension if possible.\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test if this works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (100, 10, 2)\n",
    "dummydata = torch.randint(0, 10, datashape)\n",
    "dummydataset = SunspotDataset(dummydata, horizon=2)\n",
    "len(dummydataset) # uses the __len__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dummydataset[0] # uses the __getitem__ method\n",
    "x.shape, y.shape # no squeeze because we have dimension 2 as the last dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we apply this on the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 3\n",
    "traindataset = SunspotDataset(trainset, horizon=horizon)\n",
    "testdataset = SunspotDataset(testset, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = traindataset[1]\n",
    "x.shape, y.shape # here we see the squeeze happening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 A Dataloader\n",
    "This seems to work as well. \n",
    "Again, everything is wrapped into the dataset factory, which will give you windowed and batched streamers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspotfactory.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "streamers = sunspotfactory.create_datastreamer(batchsize=32, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of all the output?\n",
    "What is going on? Is that what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trainstreamer))\n",
    "type(x), len(x), x[0].shape, type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think these numbers are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FJZ1n6GJLO7_"
   },
   "source": [
    "# 2 Exploring baseline models\n",
    "## 2.1 Naive models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The most basic prediction is a Naive model. \n",
    "\n",
    "- What is a naive prediction of the temperature for tomorrow? Well, the temperature of today.\n",
    "- The stock market for tomorrow? Same as today.\n",
    "- Bitcoin for march 16th of 2028? Same as march 15th 2028.\n",
    "\n",
    "In general, this is a super simple model without any parameters but often much better than you would expect, because the temparature of today will be close to the temperature of tomorrow.\n",
    "\n",
    "In general, you can say that f is a function that you feed the current moment, which is a real number, and it will map it to one step into the future, which is also a real number.\n",
    "\n",
    "$$ f: x_t \\in \\mathcal{R} \\to x_{t+1} \\in \\mathcal{R}$$\n",
    "\n",
    "but $f$ is just the Identity function $f(x) = x$, so one step into the future is the same as the current timestep.\n",
    "\n",
    "This gives rise to the MASE, the Mean Absolute Scaled Error: It is the mean absolute error of the forecast values (the actual model) \n",
    "$\\frac{1}{J}\\sum_j|e_j|$, but scaled by the mean absolute error of the in-sample one-step naive forecast $|Y_t-Y_{t-1}|$ (the naive model):\n",
    "\n",
    "$$ MASE = \\frac{\\frac{1}{J}\\sum_j|e_j|}{\n",
    "\\frac{1}{T-1} \\sum_{t=2}^{T}|Y_t-Y_{t-1}|} $$\n",
    "\n",
    "This takes a moment in time $t$, and calculates the difference with one step back $t-1$.\n",
    "\n",
    "In other words, the MASE compares the MAE of your actual model to the MAE of the naive model. \n",
    "\n",
    "E.g. if the error of your model is 1, and the error of the naive model is 10, your MASE is 0.1, meaning you are much better than the naive prediction.\n",
    "\n",
    "MASE values above 1 are really bad, because it means the naive function outperformed your actual model, everything below (ideally close to 0) is an improvement.\n",
    "\n",
    "We will implement this ourselves. First the naive prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.Tensor\n",
    "\n",
    "def naivepredict(x: Tensor, horizon: int) -> tuple[Tensor, Tensor]:\n",
    "    assert horizon > 0\n",
    "    yhat = x[...,-horizon:, :].squeeze(-1)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = naivepredict(x, horizon) #this should be the same horizon you used to create the dataset\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the naive prediction, we can calculate the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y: np.ndarray, yhat: np.ndarray) -> float:\n",
    "    return np.mean(np.abs(y-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(y.detach().numpy(), yhat.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is not deterministic, because of the shuffling in the dataset.\n",
    "\n",
    "Now, we want to calculate the naive MAE for every batch, to get the average, expected MAE for the naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivenorm(train , horizon: int):\n",
    "    elist = []\n",
    "    streamer = train.stream()\n",
    "\n",
    "    for _ in range(len(train)):\n",
    "        x, y = next(iter(streamer))\n",
    "        yhat = naivepredict(x, horizon)\n",
    "        e = mae(y.numpy(), yhat.numpy())\n",
    "        elist.append(e)\n",
    "    return torch.mean(torch.tensor(elist))\n",
    "naivenorm(train, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, a MAE close to this number is not impressive at all. We need to get below this if we want to go beyond the naive prediction.\n",
    "We can wrap everything in a class, so it is neatly organized.\n",
    "\n",
    "This example also illustrates two other dunder methods, `__repr__` and `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets.base import BaseDatastreamer\n",
    "Tensor = torch.Tensor\n",
    "class MASE:\n",
    "    def __init__(self, train: BaseDatastreamer, horizon: int):\n",
    "        self.scale = self.naivenorm(train, horizon)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # __repr__ is what is returned if you print the class to the screen\n",
    "        return f\"MASE(scale={self.scale:.3f})\"\n",
    "\n",
    "    def naivenorm(self, train: BaseDatastreamer, horizon: int) -> Tensor:\n",
    "        elist = []\n",
    "        streamer = train.stream()\n",
    "        for _ in range(len(train)):\n",
    "            x, y = next(iter(streamer))\n",
    "            yhat = self.naivepredict(x, horizon)\n",
    "            e = self.mae(y.numpy(), yhat.numpy())\n",
    "            elist.append(e)\n",
    "        return torch.mean(torch.tensor(elist))\n",
    "\n",
    "    def naivepredict(self, x: Tensor, horizon: int) -> Tuple[Tensor, Tensor]:\n",
    "        assert horizon > 0\n",
    "        yhat = x[...,-horizon:, :].squeeze(-1)\n",
    "        return yhat\n",
    "\n",
    "    def mae(self, y: np.ndarray, yhat: np.ndarray) -> float:\n",
    "        return np.mean(np.abs(y-yhat))\n",
    "\n",
    "    def __call__(self, y: np.ndarray, yhat: np.ndarray) -> float:\n",
    "        # __call__ lets us call the class as a function\n",
    "        return self.mae(y, yhat) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mase = MASE(train, horizon)\n",
    "mase # __repr__ in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mase(y.numpy(), yhat.numpy()) # __call__ in action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0Iz-X2A8LO8A"
   },
   "source": [
    "## 2.2 Linear model\n",
    "Ok, we have everything in place. Let's start with a slightly more advanced model than a naive one.\n",
    "\n",
    "We start with a linear baselinemodel, of just a single `Linear` layer with one unit. This is equal to a linear model.\n",
    "\n",
    "**NOTE** A Neural Network (consisting of Linear layers AND activation layers) is NOT a simple model; if your model is deep & wide enough, this is a universal function approximator! That means a neural network is capable, in theory, of approximating EVERY function. In this case we use a single layer, with a minimum amount of units; that's why this is a simple model!\n",
    "\n",
    "Check out the sourcecode (tip: VS code let's you do so easily by pressing F12, or by right-clicking the mouse on a function and selecting `Go to Definition`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import rnn_models, metrics\n",
    "observations = window_size - horizon\n",
    "model = rnn_models.BaseModel(observations=observations, horizon=horizon)\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mase = MASE(train, horizon)\n",
    "mase(y.detach().numpy(), yhat.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh boy! Untrained, this is actually much much worse than a naive model... Let's start training...\n",
    "\n",
    "First, we add a regular MAE too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae = metrics.MAE()\n",
    "mae(yhat.detach().numpy(), y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path(\"../../models/rnn/\").resolve()\n",
    "settings = TrainerSettings(\n",
    "    epochs=100,\n",
    "    metrics=[mase, mae],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN],\n",
    "    scheduler_kwargs = {\"factor\": 0.5, \"patience\": 5},\n",
    "    earlystop_kwargs = {\n",
    "        \"save\": False,\n",
    "        \"verbose\": True,\n",
    "        \"patience\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # faster than mps in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import Trainer\n",
    "import gin\n",
    "\n",
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "observations = window_size - horizon\n",
    "model = rnn_models.BaseModel(observations=observations, horizon=horizon)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )\n",
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look at Tensorboard, you can see the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(validstreamer))\n",
    "yhat = model(x)\n",
    "mae(y.detach().numpy(), yhat.detach().numpy()), mase(y.detach().numpy(), yhat.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, yes, the MASE dropped below 1! This is a good start.\n",
    "\n",
    "Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axs = ax.ravel()\n",
    "\n",
    "timesteps = x.shape[1]\n",
    "idx0 = [*range(timesteps)]\n",
    "idx1 = [(i + timesteps) for i in range(horizon)]\n",
    "for i in range(k):\n",
    "    axs[i].plot(idx0, x[i])\n",
    "    axs[i].plot(idx1, yhat[i].detach(), color=\"red\", label=\"predict\")\n",
    "    axs[i].plot(idx1, y[i], color=\"green\", label=\"actual\")\n",
    "handles, labels = axs[i].get_legend_handles_labels()\n",
    "fig.legend(handles, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it is not perfect, obviously. But it does seem to capture the trend more often that random."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bv7UFcltLO8E"
   },
   "source": [
    "## 2.3 Simple RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the basic RNN without gates.\n",
    "\n",
    "Input size is 1, because we have 1 feature. We follow [the batch_first convention](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). Hidden size is the inner dimension of the RNN, and thus the dimensionality of the output.\n",
    "\n",
    "What would happen if we feed this data with dimensions `(batch, sequence_length)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 25))\n",
    "rnn = torch.nn.RNN(input_size=1, hidden_size=1, batch_first=True)\n",
    "try:\n",
    "    yhat = rnn(x)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it expects 3 dimensions : `(batch, sequence_length, features)`, even though we have just one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = rnn(x)\n",
    "out.shape, hidden.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected. Let's increase the hidden dimension (why would you do that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=1, hidden_size=10, batch_first=True)\n",
    "out, hidden = rnn(x)\n",
    "out.shape, hidden.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add multiple stacked layers of RNN (can you visualize how this would work?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(\n",
    "    input_size=1,\n",
    "    hidden_size=10,\n",
    "    num_layers=3,\n",
    "    batch_first=True)\n",
    "out, hidden = rnn(x)\n",
    "out.shape, hidden.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the output of the last timestep, we need to throw away the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[:,-1,:]\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's wrap this up in a model. Study the implementation in `rnn_models.BaseRNN`.\n",
    "\n",
    "Do you understand what the linear layer does? What would happen if you remove it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import metrics, rnn_models\n",
    "\n",
    "mase = metrics.MASE(train, horizon)\n",
    "mae = metrics.MAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.epochs = 10\n",
    "device = \"cpu\" # still faster than mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "observations = window_size - horizon\n",
    "model = rnn_models.BaseRNN(horizon=horizon)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )\n",
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this improve over the linear model?\n",
    "By how much? Is there a difference between loss and MASE? \n",
    "\n",
    "Use the tensorboard to find this out!\n",
    "\n",
    "What does it mean?\n",
    "\n",
    "Can you improve the model by tweaking it?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
