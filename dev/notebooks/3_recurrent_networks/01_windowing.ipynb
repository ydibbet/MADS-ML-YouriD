{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10, 100, 10)\n",
    "print(f\"Length of x: {len(x)}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let this be a timeseries with 9 observations.\n",
    "Now, often, it is the case that you dont need all the observations to make a prediction.\n",
    "\n",
    "Imagine you are predicting weather: is the weather from 4 months ago relevant for the prediction of tomorrow? Probably not that much.\n",
    "So, typically, we will have to determine a relevant window of time. \n",
    "\n",
    "This will vary from case to case. In this case, let the optimal window be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([0, 1, 2])\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first training example. But, because time flows, we could \"simulate\" this by extracting an additional training example, also of lenght 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 30, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([1, 2, 3])\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can continue this all the way to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70, 80, 90])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([6, 7, 8])\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be easy to scale this process. So, we can provide multiple indeces at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30],\n",
       "        [20, 30, 40]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 2],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us two different training examples.\n",
    "Now, what we would want, instead of typing out these indeces by hand, is to generate a complete set of indeces, for an array of a given lenght, with a given window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_time = 3\n",
    "n_window = len(x) - n_time + 1\n",
    "n_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum case is to extract one window of 3 from an array of length 3, which is the reason we need to add 1 to the formula.\n",
    "If we increase the length of the array, for every single element extra on top of the length of the array, we can extract one more chunk.\n",
    "This explains the len(x) - n_time part.\n",
    "\n",
    "This formula can be used to calculate the maximum amount of slices we can extract from an array `x`: with a window of 3, we can squeeze out 7 training examples.\n",
    "\n",
    "Now, the first index will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = torch.arange(0, n_time).reshape(1, -1)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what we essentially need, to get the next slice, is to add +1 for every next slice.\n",
    "Because we calculated that we are able to extract 7 slices, we need the numbers ranging from 0 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = torch.arange(0, n_window).reshape(-1, 1)\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([7, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape, window.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using broadcasting, we can now simply add these two vectors. \n",
    "\n",
    "We are adding a (1,3) and a (7,1) matrix. This might seem weird, because you would expect to need two\n",
    "(7,3) matrices, but Torch will expand the dimensions that don't match yet have a dimension of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [3, 4, 5],\n",
       "        [4, 5, 6],\n",
       "        [5, 6, 7],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = time + window\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is exactly what we need! This first index is still `[0, 1, 2]`, and the second index has the same, +1, so that is now `[1, 2, 3]` , etc.\n",
    "\n",
    "Let's try this out on our timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30],\n",
       "        [20, 30, 40],\n",
       "        [30, 40, 50],\n",
       "        [40, 50, 60],\n",
       "        [50, 60, 70],\n",
       "        [60, 70, 80],\n",
       "        [70, 80, 90]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that worked! We started with a long timeseries of 9 steps. We ended up with seven examples to feed our model, all with a lenght of 3.\n",
    "\n",
    "We can wrap this all into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.Tensor\n",
    "def window(x: Tensor, n_time: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates and index that can be used to window a timeseries.\n",
    "    E.g. the single series [0, 1, 2, 3, 4, 5] can be windowed into 4 timeseries with\n",
    "    length 3 like this:\n",
    "\n",
    "    [0, 1, 2]\n",
    "    [1, 2, 3]\n",
    "    [2, 3, 4]\n",
    "    [3, 4, 5]\n",
    "\n",
    "    We now can feed 4 different timeseries into the model, instead of 1, all\n",
    "    with the same length.\n",
    "    \"\"\"\n",
    "    n_window = len(x) - n_time + 1\n",
    "    time = torch.arange(0, n_time).reshape(1, -1)\n",
    "    window = torch.arange(0, n_window).reshape(-1, 1)\n",
    "    idx = time + window\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily change the window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30, 40, 50],\n",
       "        [20, 30, 40, 50, 60],\n",
       "        [30, 40, 50, 60, 70],\n",
       "        [40, 50, 60, 70, 80],\n",
       "        [50, 60, 70, 80, 90]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = window(x, 5)\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30, 40, 50, 60],\n",
       "        [20, 30, 40, 50, 60, 70],\n",
       "        [30, 40, 50, 60, 70, 80],\n",
       "        [40, 50, 60, 70, 80, 90]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = window(x, 6)\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this scale to more dimensions?\n",
    "Let's imagine we have 5 timesteps, every timestep 3 features being observed. \n",
    "We can organize that into a `(5x3)` matrix instead of the one-dimensional raw data we started this notebook with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 8, 9],\n",
       "        [9, 8, 9],\n",
       "        [0, 7, 9],\n",
       "        [9, 2, 1],\n",
       "        [7, 7, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 10, (5, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets window it in chunks of 4 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7, 8, 9],\n",
       "         [9, 8, 9],\n",
       "         [0, 7, 9],\n",
       "         [9, 2, 1]],\n",
       "\n",
       "        [[9, 8, 9],\n",
       "         [0, 7, 9],\n",
       "         [9, 2, 1],\n",
       "         [7, 7, 5]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = window(x, 4)\n",
    "x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the windowed index to generate three training examples, every training examples covering four timesteps.\n",
    "\n",
    "We can also apply this to batched. Let us have a batch (B) of 32 examples, every example having 6 timesteps (T) and 2 features (F). This is organized in a `(B, T, F)` matrix.\n",
    "\n",
    "We can now apply the window on the 0th example, and squueze out an additional 3 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.randint(0, 10, (32, 6, 2))\n",
    "x = batch[0]\n",
    "idx = window(x, 4)\n",
    "x_windowed = x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([3, 4, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_windowed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep-learning-wM7qE7ca-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
