{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Intro into torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the basics. We create some dummy dataset: two observations $n$, every observation has three features $m$. We organize that as a dataset with dimensions $(n,m)$, so that is $(2,3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, 2, 3],\n",
    "    [10, 20, 30]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our datatype here is `List[int]`, and PyTorch uses a `torch.Tensor` datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(data)\n",
    "type(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the type of the data inside the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the amount of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start with a `numpy.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata = np.array(\n",
    "    data,\n",
    "    dtype = np.float32\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we changed the dataformat to `np.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [10., 20., 30.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = torch.from_numpy(npdata)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Usefull functions for creating tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create a stand in tensor, with the same shape as our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones_like(X2)\n",
    "ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or random weights. These are uniform distributed positive numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2482, 0.4569, 0.2177],\n",
       "        [0.1037, 0.5657, 0.7017]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = torch.rand(2,3)\n",
    "X3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want normally distributed numbers, we need to specify mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0385,  0.1704,  0.1568],\n",
       "        [-0.0308,  0.0604,  0.0668]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4 = torch.normal(mean=0.0, std=0.1, size=(2,3))\n",
    "X4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your laptop or server has a GPU, PyTorch can run the calculations on the GPU. You can check if the GPU can be found by PyTorch with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can set the tensor to the GPU device with `.to()`. Default is `\"cpu\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = X3.to(\"cuda\")\n",
    "else:\n",
    "    print(\"cuda not found\")\n",
    "X3.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For people with a macbook with an `mps` backend, there is mps acceleration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2482, 0.4569, 0.2177],\n",
       "        [0.1037, 0.5657, 0.7017]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device {device}\")\n",
    "tensor = X3.to(device)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that using accelaration with cuda or mps is not always faster!\n",
    "Reasons why this can be slower are:\n",
    "- Memory transer: data needs to be transfered from cpu to gpu. This can be a bottleneck.\n",
    "- parallel processing limits: some architectures (especially the RNNs we will learn about in lesson 3) cant be parallelized. \n",
    "- synchronisation overhead: running things in parallel also takes some overhead to synchronise the calculations, like waiting things to finish, merging them back together, etc.\n",
    "\n",
    "This will especially be true for the simplere models and datasets we are using in the contexts of our lessons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other usefull tricks are to create an array of ones. Can you figure out how to create an array of zeros for yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(1, 10)\n",
    "ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be concatenated. We need to specify the dimension along which the concatenation is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([ones, ones, ones], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Manipulation of tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of most machine learning functions is the linear function. We can easily scale this by using matrix multiplication. Let's say we start with some random data, 32 observations with 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(32, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want a linear map that transforms these 10 features into 2 dimensions, we can do that with a set of weights with dimensions $(10,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = X @ W\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent is this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = torch.matmul(X, W)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch will scale this up if you have more dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(32, 10, 16)\n",
    "W = torch.rand(16, 2)\n",
    "yhat = X @ W\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can aggregate the tensor along the two features by taking the mean over the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate = yhat.mean(dim=-1)\n",
    "aggregate.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try for yourself to calculate the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.4675, 5.8201],\n",
       "         [3.3638, 3.7567],\n",
       "         [3.3921, 3.1681],\n",
       "         [3.8308, 4.7772],\n",
       "         [4.0095, 4.9362],\n",
       "         [3.8744, 4.6699],\n",
       "         [4.1687, 4.9073],\n",
       "         [1.9838, 2.5658],\n",
       "         [3.4768, 3.5188],\n",
       "         [2.5056, 3.4572]],\n",
       "\n",
       "        [[3.0957, 4.4960],\n",
       "         [4.0896, 4.0480],\n",
       "         [3.1695, 4.5845],\n",
       "         [4.4032, 5.4913],\n",
       "         [3.1172, 5.0961],\n",
       "         [3.9466, 5.5266],\n",
       "         [2.8967, 3.3648],\n",
       "         [2.8598, 3.8175],\n",
       "         [3.8294, 4.3014],\n",
       "         [3.0703, 3.8020]],\n",
       "\n",
       "        [[4.0340, 5.2569],\n",
       "         [3.4419, 4.7061],\n",
       "         [3.1453, 4.1779],\n",
       "         [4.2146, 4.6199],\n",
       "         [4.0382, 5.1759],\n",
       "         [3.4342, 4.0966],\n",
       "         [3.4971, 4.4101],\n",
       "         [3.7160, 4.7781],\n",
       "         [3.4030, 3.9804],\n",
       "         [1.9648, 2.6012]],\n",
       "\n",
       "        [[3.5888, 4.6670],\n",
       "         [2.3334, 2.8457],\n",
       "         [2.9569, 3.7064],\n",
       "         [4.4695, 5.7450],\n",
       "         [2.8652, 4.3201],\n",
       "         [3.1238, 3.9885],\n",
       "         [3.8315, 5.4937],\n",
       "         [2.6714, 4.1039],\n",
       "         [2.6162, 3.4807],\n",
       "         [4.3651, 5.1724]],\n",
       "\n",
       "        [[3.3704, 4.6588],\n",
       "         [3.6084, 4.7514],\n",
       "         [3.2419, 4.0507],\n",
       "         [2.5928, 3.7791],\n",
       "         [2.3227, 3.9221],\n",
       "         [3.2252, 3.6172],\n",
       "         [2.5914, 3.6249],\n",
       "         [4.4863, 5.2688],\n",
       "         [3.5843, 4.5759],\n",
       "         [4.1661, 5.2665]],\n",
       "\n",
       "        [[2.7123, 4.3342],\n",
       "         [3.6842, 5.0204],\n",
       "         [2.8721, 4.5266],\n",
       "         [2.6025, 3.4313],\n",
       "         [4.6307, 5.7809],\n",
       "         [3.5508, 4.0468],\n",
       "         [3.9732, 4.2490],\n",
       "         [2.9984, 4.6002],\n",
       "         [3.1273, 4.0523],\n",
       "         [2.9852, 3.6491]],\n",
       "\n",
       "        [[2.9657, 3.0759],\n",
       "         [3.4460, 4.5792],\n",
       "         [3.6270, 3.9555],\n",
       "         [2.2712, 3.7743],\n",
       "         [3.5648, 4.4509],\n",
       "         [3.9779, 3.9023],\n",
       "         [2.9142, 3.9131],\n",
       "         [3.7254, 4.9685],\n",
       "         [3.5627, 4.1611],\n",
       "         [4.0696, 4.8420]],\n",
       "\n",
       "        [[3.5664, 5.5933],\n",
       "         [3.0388, 2.9920],\n",
       "         [3.3490, 3.7837],\n",
       "         [4.0844, 4.2156],\n",
       "         [3.5597, 4.6674],\n",
       "         [2.5422, 4.2496],\n",
       "         [2.8250, 3.9638],\n",
       "         [3.2784, 4.5830],\n",
       "         [3.5017, 4.4753],\n",
       "         [3.5098, 3.7508]],\n",
       "\n",
       "        [[3.7441, 4.5124],\n",
       "         [2.9185, 3.7723],\n",
       "         [3.4077, 4.7041],\n",
       "         [2.5744, 3.4271],\n",
       "         [3.6231, 4.3340],\n",
       "         [4.1923, 5.6457],\n",
       "         [2.9080, 3.8474],\n",
       "         [2.7808, 4.0443],\n",
       "         [2.8316, 3.7992],\n",
       "         [3.5827, 4.5582]],\n",
       "\n",
       "        [[2.6882, 3.1545],\n",
       "         [3.1127, 4.7670],\n",
       "         [3.4874, 3.8943],\n",
       "         [3.7244, 4.2936],\n",
       "         [3.6255, 4.7888],\n",
       "         [2.7406, 3.8668],\n",
       "         [3.9031, 4.8917],\n",
       "         [3.6938, 4.2498],\n",
       "         [3.5771, 3.7714],\n",
       "         [4.3446, 5.2889]],\n",
       "\n",
       "        [[3.5951, 5.4932],\n",
       "         [3.6658, 4.5333],\n",
       "         [3.6320, 4.7939],\n",
       "         [3.0792, 3.5837],\n",
       "         [2.6508, 3.5585],\n",
       "         [3.0717, 3.9801],\n",
       "         [4.0014, 4.8354],\n",
       "         [3.9894, 4.9422],\n",
       "         [4.1733, 4.5707],\n",
       "         [3.9419, 4.8498]],\n",
       "\n",
       "        [[4.5342, 5.3631],\n",
       "         [2.4605, 3.0590],\n",
       "         [3.0437, 4.3705],\n",
       "         [3.0100, 3.9031],\n",
       "         [3.7808, 4.2148],\n",
       "         [2.9399, 4.2515],\n",
       "         [2.1683, 3.0772],\n",
       "         [4.4253, 5.6238],\n",
       "         [3.9664, 5.7092],\n",
       "         [3.1503, 3.7110]],\n",
       "\n",
       "        [[4.2825, 4.8286],\n",
       "         [2.9869, 4.1614],\n",
       "         [3.8341, 4.9880],\n",
       "         [3.6723, 5.0657],\n",
       "         [3.4044, 3.9681],\n",
       "         [3.4972, 3.5784],\n",
       "         [3.3847, 4.4988],\n",
       "         [3.0362, 3.8095],\n",
       "         [2.4895, 2.4758],\n",
       "         [3.8169, 4.6198]],\n",
       "\n",
       "        [[3.0723, 2.9797],\n",
       "         [2.5809, 3.5248],\n",
       "         [3.6141, 3.5247],\n",
       "         [4.4868, 5.0899],\n",
       "         [3.4898, 4.8860],\n",
       "         [2.7540, 3.3495],\n",
       "         [3.2678, 3.8129],\n",
       "         [3.6402, 4.2310],\n",
       "         [4.2916, 5.3904],\n",
       "         [3.8548, 4.5476]],\n",
       "\n",
       "        [[3.9205, 3.8136],\n",
       "         [4.4485, 5.0805],\n",
       "         [3.6021, 4.8944],\n",
       "         [3.3073, 4.4061],\n",
       "         [4.1884, 4.6673],\n",
       "         [2.6845, 3.7601],\n",
       "         [3.1291, 4.7810],\n",
       "         [3.2614, 4.4624],\n",
       "         [4.0054, 4.7431],\n",
       "         [3.5707, 3.9842]],\n",
       "\n",
       "        [[3.2947, 4.9957],\n",
       "         [4.8202, 5.9729],\n",
       "         [3.8379, 4.4514],\n",
       "         [4.5740, 5.3738],\n",
       "         [3.6063, 4.3514],\n",
       "         [3.4545, 4.8744],\n",
       "         [3.2421, 4.2355],\n",
       "         [2.5475, 3.9100],\n",
       "         [2.5303, 3.3208],\n",
       "         [3.8217, 4.3382]],\n",
       "\n",
       "        [[2.6899, 3.9232],\n",
       "         [3.8948, 5.0490],\n",
       "         [3.2893, 4.1493],\n",
       "         [3.2312, 3.7393],\n",
       "         [3.9128, 5.1730],\n",
       "         [3.6601, 4.1310],\n",
       "         [3.3056, 4.6513],\n",
       "         [3.8555, 4.6220],\n",
       "         [3.4473, 4.8507],\n",
       "         [3.7553, 4.6946]],\n",
       "\n",
       "        [[3.8811, 4.7672],\n",
       "         [3.5959, 4.4470],\n",
       "         [2.9645, 4.6276],\n",
       "         [2.6160, 3.6676],\n",
       "         [3.7726, 4.3763],\n",
       "         [3.3352, 3.8261],\n",
       "         [3.4400, 4.2415],\n",
       "         [3.4199, 4.3377],\n",
       "         [3.3076, 3.5759],\n",
       "         [4.1696, 4.4733]],\n",
       "\n",
       "        [[2.6224, 3.3830],\n",
       "         [3.3504, 5.0616],\n",
       "         [3.0663, 4.2067],\n",
       "         [3.4986, 4.6889],\n",
       "         [3.3936, 4.4708],\n",
       "         [2.7339, 3.7769],\n",
       "         [2.7406, 3.4342],\n",
       "         [3.3792, 3.2202],\n",
       "         [3.6516, 4.3174],\n",
       "         [3.8709, 5.6045]],\n",
       "\n",
       "        [[3.0468, 3.8727],\n",
       "         [4.3966, 4.6438],\n",
       "         [3.1674, 3.6283],\n",
       "         [2.7165, 3.7008],\n",
       "         [4.2802, 4.6891],\n",
       "         [4.8062, 5.4294],\n",
       "         [4.5371, 5.7379],\n",
       "         [4.4586, 5.2531],\n",
       "         [4.0509, 4.8234],\n",
       "         [3.8168, 5.4419]],\n",
       "\n",
       "        [[3.8066, 4.4523],\n",
       "         [3.1701, 3.6747],\n",
       "         [2.9836, 3.0049],\n",
       "         [3.2735, 4.3418],\n",
       "         [3.2318, 4.3195],\n",
       "         [3.3017, 4.5344],\n",
       "         [3.5189, 4.6185],\n",
       "         [4.4652, 5.5778],\n",
       "         [2.9603, 3.3547],\n",
       "         [2.8345, 3.4678]],\n",
       "\n",
       "        [[3.3977, 4.1589],\n",
       "         [2.9666, 3.6507],\n",
       "         [3.1103, 3.9259],\n",
       "         [3.1071, 5.0798],\n",
       "         [4.4101, 4.9328],\n",
       "         [2.7008, 3.1093],\n",
       "         [3.1567, 4.2265],\n",
       "         [3.3449, 4.2376],\n",
       "         [3.5327, 3.9618],\n",
       "         [3.2865, 4.3501]],\n",
       "\n",
       "        [[4.2065, 4.3879],\n",
       "         [4.2658, 5.6342],\n",
       "         [3.4571, 4.4400],\n",
       "         [4.4584, 5.2882],\n",
       "         [3.8203, 4.8570],\n",
       "         [4.4084, 5.3881],\n",
       "         [3.0362, 4.5048],\n",
       "         [4.0522, 4.7990],\n",
       "         [3.7836, 5.4666],\n",
       "         [3.3091, 4.0751]],\n",
       "\n",
       "        [[2.2255, 3.3950],\n",
       "         [2.3250, 3.8621],\n",
       "         [3.9199, 5.0099],\n",
       "         [3.3445, 3.8929],\n",
       "         [3.4860, 4.8006],\n",
       "         [3.1855, 4.1266],\n",
       "         [3.2655, 3.9466],\n",
       "         [2.9758, 2.7757],\n",
       "         [3.5927, 4.8188],\n",
       "         [4.1829, 5.1427]],\n",
       "\n",
       "        [[3.1764, 3.8509],\n",
       "         [4.0402, 4.3199],\n",
       "         [3.3027, 3.7186],\n",
       "         [4.2747, 5.7478],\n",
       "         [3.0388, 3.0698],\n",
       "         [2.6412, 3.6765],\n",
       "         [2.7715, 4.0160],\n",
       "         [3.6120, 4.5506],\n",
       "         [3.0291, 3.9687],\n",
       "         [3.1178, 3.4863]],\n",
       "\n",
       "        [[3.5042, 3.8332],\n",
       "         [3.2847, 4.0179],\n",
       "         [3.2208, 4.3632],\n",
       "         [2.8576, 4.5585],\n",
       "         [2.5442, 3.8334],\n",
       "         [3.5417, 4.5384],\n",
       "         [3.6649, 4.1551],\n",
       "         [3.2581, 4.1697],\n",
       "         [3.3156, 4.7811],\n",
       "         [4.1551, 4.8024]],\n",
       "\n",
       "        [[2.8803, 3.5359],\n",
       "         [2.1231, 2.4444],\n",
       "         [3.2982, 3.8050],\n",
       "         [3.3787, 4.4144],\n",
       "         [3.1214, 3.9294],\n",
       "         [3.3055, 4.1805],\n",
       "         [4.6106, 5.4117],\n",
       "         [2.7800, 3.3121],\n",
       "         [4.5328, 5.0350],\n",
       "         [3.2537, 3.9501]],\n",
       "\n",
       "        [[3.3674, 4.7256],\n",
       "         [3.7294, 4.8527],\n",
       "         [3.6183, 4.7142],\n",
       "         [3.8518, 4.1225],\n",
       "         [2.7988, 3.9677],\n",
       "         [3.8811, 4.0765],\n",
       "         [3.6774, 3.4165],\n",
       "         [4.3292, 5.6221],\n",
       "         [3.2209, 4.1647],\n",
       "         [3.9171, 5.4714]],\n",
       "\n",
       "        [[3.4194, 5.4037],\n",
       "         [3.8373, 3.9758],\n",
       "         [3.0839, 5.0464],\n",
       "         [3.3877, 3.6969],\n",
       "         [3.4503, 5.4327],\n",
       "         [4.3061, 4.9316],\n",
       "         [4.1548, 4.7245],\n",
       "         [3.1367, 3.5808],\n",
       "         [2.4143, 3.9955],\n",
       "         [3.7312, 4.7787]],\n",
       "\n",
       "        [[3.9540, 4.8462],\n",
       "         [2.6789, 3.4839],\n",
       "         [4.3180, 5.1318],\n",
       "         [3.2684, 3.8092],\n",
       "         [2.9840, 3.2675],\n",
       "         [3.4378, 4.8181],\n",
       "         [3.2077, 4.1953],\n",
       "         [3.9232, 3.8998],\n",
       "         [3.2386, 3.5438],\n",
       "         [3.6708, 4.5847]],\n",
       "\n",
       "        [[2.8163, 3.9919],\n",
       "         [4.4235, 5.0588],\n",
       "         [4.2892, 5.3988],\n",
       "         [2.8500, 4.3089],\n",
       "         [2.2474, 3.3383],\n",
       "         [3.0877, 3.6468],\n",
       "         [4.0960, 4.8794],\n",
       "         [3.4122, 3.3831],\n",
       "         [3.7024, 4.4382],\n",
       "         [2.5174, 3.7962]],\n",
       "\n",
       "        [[3.3323, 4.0469],\n",
       "         [3.6090, 4.3291],\n",
       "         [2.8521, 3.4151],\n",
       "         [3.1244, 3.9555],\n",
       "         [4.0385, 4.5361],\n",
       "         [4.0010, 4.8297],\n",
       "         [3.2181, 4.0169],\n",
       "         [2.1798, 3.8300],\n",
       "         [3.7123, 4.7087],\n",
       "         [3.4086, 4.6482]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = yhat.sum(dim=2)\n",
    "sum.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 GPU or CPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors live in the CPU or GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if you have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a mac with M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And move a tensor to the GPU for faster computing, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    X_ = X.to(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    X_ = X.to(\"mps\")\n",
    "else:\n",
    "    X_ = X.to(\"cpu\")\n",
    "X_.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Reshape or View"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you will need to reshape a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28, 1]), torch.Size([32, 784]), torch.Size([32, 784]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(32, 28, 28, 1)\n",
    "X_view = X.view(32, 28*28)\n",
    "X_reshape = X.reshape(32, 28*28)\n",
    "X.shape, X_view.shape, X_reshape.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `view` and `reshape` is: `view` operates as a view on the original tensor. If the underlying data is changed, the view will change too.\n",
    "\n",
    "No data movement occurs when creating a view, view tensor just changes the way it interprets the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YouriDibbet\\AppData\\Local\\Temp\\ipykernel_21648\\3787215844.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  X.storage().data_ptr() == X_view.storage().data_ptr()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor([0, 0])\n",
    "X_view = X.view(1,2)\n",
    "X.storage().data_ptr() == X_view.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] = 1\n",
    "X_view"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`view` can throw an error if the required view is not contiguous (does not share the same memory block)\n",
    "\n",
    "> A tensor whose values are laid out in the storage starting from the rightmost dimension onward (that is, moving along rows for a 2D tensor) is defined as contiguous. Contiguous tensors are convenient because we can visit them efficiently in order without jumping around in the storage (improving data locality improves performance because of the way memory access works on modern CPUs). This advantage of course depends on the way algorithms visit.\n",
    "\n",
    "You could call `.contiugous()` on a `view`, but `.reshape()` does that behind the scenes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Permute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might want to reshuffle the order of a tensor.\n",
    "\n",
    "For example, let's say we load an batch of 32 images, where every image has a size of 28x28 pixels, and has 3 channels (RGB color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(32, 28, 28, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the case that there are different conventions for manipulating tensors in image recognition models. Some models have a channel-last convention, like I used above, but some (like [pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)) use a channel first convention, which would be (batch, channel, height, width).\n",
    "\n",
    "You would want to swap the 4th dimension to the 2nd, or if you start from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_first = X.permute(0, 3, 1, 2)\n",
    "channel_first.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Broadcasting\n",
    "\n",
    "Broadcasting is something you might know from `numpy`, but it is also used by `tensorflow`, `jax` and `torch`. \n",
    "\n",
    "Broadcasting allows to extend a dimension, without the need to do so explicitly. The rules for broadcasting are simple:\n",
    "\n",
    "- two dimesions are equal\n",
    "- one of the dimensions is 1\n",
    "\n",
    "but lets show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[2., 2.],\n",
       "         [2., 2.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.ones(2, 2)\n",
    "a, b, a+b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is straigh forward. But what would happen in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1, 2)\n",
    "b = torch.ones(2, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b` is a 2x2 grid, and has four numbers. If we want to add `a`, we have only two numbers! Now, you could start stacking the `a` tensor to get matching dimensions. But you dont have to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happened here? \n",
    "\n",
    "`a` is magically broadcasted over the first dimension. And what would you guess would happen in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1, 5, 1, 4)\n",
    "b = torch.ones(3, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       "\n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]],\n",
       "\n",
       "\n",
       "        [[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       "\n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]],\n",
       "\n",
       "\n",
       "        [[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       "\n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 3, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a + b).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, predict the output shape, then check it for yourself.\n",
    "\n",
    "And, what would you think happens here; do you think this gives an error, or do you think it broadcasts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5, 2, 4, 1)\n",
    "b = torch.ones(5, 1, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]],\n",
       "\n",
       "         [[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]],\n",
       "\n",
       "         [[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]],\n",
       "\n",
       "         [[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]],\n",
       "\n",
       "         [[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]],\n",
       "\n",
       "         [[2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.],\n",
       "          [2., 2.]]]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
