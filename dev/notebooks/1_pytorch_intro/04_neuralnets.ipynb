{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Non Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear models\n",
    "One of the simplest models is the linear model. The basic shape of the formula is:\n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "Here, $x$ is a variable, $w$ and $b$ are constants that determine the slope and intercept of the line.\n",
    "\n",
    "We can scale this formula for multiple variables, like this:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Because it is easier to do this calculation as a dotproduct, we will typically use matrices.\n",
    "\n",
    "So, let us have $n$ observations of $m$ features, and we will represent that in a matrix $X$ with dimensions $(n,m)$. We will generate random observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 3\n",
    "X = torch.rand(n, m)\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix $X$ is our data. Now we want to have $m$ weights, and we want to sum that up untill we have a single number, so our weight $W$ will have dimensions $(m, 1)$. To get matrix multiplication to work, we need the last dimension of $X$ and the first dimension of $W$ to match. So because $X$ has 3 features (last dimension of $X$)$ our $W$ needs to have 3 numbers as its first dimension for `jnp.dot(X, W)` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.normal(0., 0.1, (m, 1))\n",
    "b = torch.normal(0, 0.1, (1,))\n",
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape, b.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a forloop to calculate this linear formula $n$ times, we will use a dotproduct. This will calcute \n",
    "\n",
    "$$ \n",
    "y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "$$\n",
    "\n",
    "for all $n$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = torch.matmul(X, W) + b\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do is to learn the right weights, and we have a linear model. \n",
    "We could easily generate a 2 dimensional output instead of a 1 dimensional output by increasing the dimension of $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.normal(0., 0.1, (m, 2))\n",
    "yhat = torch.matmul(X, W)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Non linear models\n",
    "However, the simplicity of linear models is both their strength and weakness:\n",
    "- The simplicity protects against overfitting\n",
    "- The simplicity will underfit on non-linear datasets\n",
    "\n",
    "While a lot of things can be approached as a linear model, there are often more complex and non-linear relations in datasets. A naive idea could be to chain multiple linear models together, like this:\n",
    "\n",
    "$$\n",
    "y = f_2(f_1(x))\n",
    "$$\n",
    "\n",
    "which is equal to:\n",
    "\n",
    "$$\n",
    "y = W_2 (W_1 X + b_1) + b_2\n",
    "$$\n",
    "\n",
    "The idea is simply to feed the output of one model to the next. However, the problem here will be that you will end up with something that is equal to a single linear model:\n",
    "\n",
    "\\begin{align}\n",
    "y &= W_2 W_1 X + W_2 b_1 + b_2\\\\\n",
    "y &= W_3 X + b_3\\\\\n",
    "\\end{align}\n",
    "\n",
    "If you simplify $W_2 W_1$ to $W_3$ and $W_2 b_1 + b_2$ to $b_3$. \n",
    "\n",
    "## 3 Activation functions\n",
    "However, what will work for chaining is to add an activation function. There are a lot of functions that will work as an activation function; the main requirement is that they are non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "x = torch.linspace(-5,5, k)\n",
    "sns.lineplot(x=x, y=sigmoid(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, you can understand an activation function as a threshold. For example, the sensation of your chair usually lies below the threshold for noticing the sensation. If you increase the input (on the x-axis) you will get above the threshold as output (on the y-axis) at some point.  \n",
    "\n",
    "There are many types of activation functions. The simplest one is the ReLu, which is 0 if $x \\le 0$ and the identity function otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.maximum(torch.Tensor([0.]), x)\n",
    "\n",
    "sns.lineplot(x=x, y=relu(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us all the building blocks needed for a neural network:\n",
    "\n",
    "![neural net](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/2_i1cdwq.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, every observation has $n$ features, in this case three.\n",
    "Every input has a corresponding weight $w$, which is summed up with a linear model. \n",
    "The output of the linear model is the fed through an activation function.\n",
    "\n",
    "Let's test that. First we will generate Tuples $(W, b)$. We will make sure the dimensions of output and input will match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def init_weights(\n",
    "    m: int, n: int, scale: float = 1e-1, seed: int = 42\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    W = scale * torch.normal(0.0, scale, (m, n))\n",
    "    b = scale * torch.normal(0.0, scale, (n,))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = init_weights(10, 2)\n",
    "W.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def torch_network(\n",
    "    sizes: List[int],\n",
    "    scale: float = 1e-1,\n",
    "    seed: int = 42,\n",
    ") -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "\n",
    "    params = [init_weights(m, n, scale, seed) for m, n in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layersizes = [2, 8, 2, 1]\n",
    "params = torch_network(layersizes, seed=42)\n",
    "for w,_ in params:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,b in params:\n",
    "    print(w, \"\\n\")\n",
    "    print(b, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three matrices. The first receives 2 features as input, 8 as output with dimensions $(2,8)$, the second will receive these 8 features as input, 2 as output and thus has $(8,2)$ dimensions, and the final layer transforms 2 dimensions into 1.\n",
    "\n",
    "This would similar to $f_1(f_2(f_3(x)))$, so chaining 3 models. However, because chaining linear models adds nothing, this could be reduced to $f(x)$ where $f$ is a linear model.\n",
    "\n",
    "Let's visualize this. First, I will build a 2D grid on the domain $[0,1]$ with a granularity of 100, so I will have $100*100$ coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def build_grid(k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"creates a grid on [0,1] domain\n",
    "    with a granularity k\n",
    "\n",
    "    Args:\n",
    "        k (int): granularity\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D coordinate grid on [0,1]\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, k)\n",
    "    y = np.linspace(0, 1, k)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    grid = np.c_[xv.ravel(), yv.ravel()]\n",
    "    return grid, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "grid, x, y = build_grid(k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[:3] # grid has coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[-3:] # from (0,0) up to (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, x[:5] # x and y contain k values on the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.from_numpy(grid).type(torch.float32)\n",
    "W, b = params[0]\n",
    "W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "def predict(params, activations):\n",
    "    for w, b in params:\n",
    "        outputs = torch.matmul(activations, w) + b\n",
    "        logger.info(f\"Shape: {outputs.shape}\")\n",
    "        activations = outputs\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "out = predict(params, grid)\n",
    "z = out.reshape(k,k)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Surface(x=x, y=y, z=z))\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what we expected. We have a 2D hyperplane. If we are doing classification, this hyperplane splits the 3D datapoints into two classes, and we could classify the group of point that fall above or below the hyperplane as belonging to one or another class.\n",
    "\n",
    "In the case of regression, we would want the point to fall on the hyperplane, as much as possible.\n",
    "\n",
    "Even though we could scale this to more dimensions easily, the limitations of a linear model should be clear: a lot of data simply is not linear.\n",
    "\n",
    "To avoid this, we can add tricks to our model:\n",
    "\n",
    "- a simple approach would be to do basis expansion. E.g. points $X = (x_1, x_2)$ might not be linear serparable, but $\\phi(X) = (x_1, x_2, x_1 x_2, x_1^2, x_2^2)$ is a basis expansion that might be linear.\n",
    "- because basis expansion runs into scaling problems pretty fast (it scales roughly $O(n^2)$) we could use the kernel trick, like a Support Vector Machine does. \n",
    "- We could use a decision tree or randomm model, which is a way to chain linear models that result in non linear models) we could use the kernel trick, like a Support Vector Machine does. \n",
    "- We could use a decision tree or randomm model, which is a way to chain linear models that result in non linear models.\n",
    "\n",
    "These \"tricks\" cover most of the classic models you can find in the scikit-learn library. While they have advantages, there will still be cases where these models are not commplex enough. \n",
    "\n",
    "# Neural Networks\n",
    "A neural network chains linear models with an activation function. The only thing that is different in the next model is that after every linear transformation we will apply the relu function.\n",
    "\n",
    "![full network](../../reports/figures/nnet.png)\n",
    "\n",
    "This is what a fully connected neural network would look like.\n",
    "We can recognize the single units with the multiple inputs.\n",
    "\n",
    "Rebuild the parameters a few times and see how the model changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersizes = [2, 8, 16, 4, 1]\n",
    "random_seed = 5\n",
    "params = torch_network(layersizes, scale=1e-1, seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(params, activations):\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = torch.matmul(activations, w) + b\n",
    "        logger.info(f\"Shape: {outputs.shape}\")\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = torch.matmul(activations, final_w) + final_b\n",
    "    logger.info(f\"Shape: {logits.shape}\")\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nn_predict(params, grid)\n",
    "z = out.reshape(k,k)\n",
    "fig = go.Figure(data=go.Surface(x=x, y=y, z=z))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila! We have a non-linear model. We will need to add a learning process, so that the model can learn to adjust its weights in order to fit the training data, but this is the basics of a neural network.\n",
    "\n",
    "Change the random_seed to see the impact of different starting values.\n",
    "Also, experiment with the sizes and shapes of the weights.\n",
    "\n",
    "\n",
    "- **TAKE AWAY 1**: A neural network is a combination of linear models and activation functions\n",
    "- **TAKE AWAY 2**: Neural networks are in theory *universal function approximators*. So never NEVER never say that a combination of Linear layers + activation functions is a \"simple model\". They can approximate EVERY function, you just need to make the network deep & wide enough and train it for long enough!\n",
    "- **TAKE AWAY 3**: However, you CAN make neural networks very simple. The simplest case is a single linear layer with no activation, which is just a linear model. If you reduce the amount of units, the model will also be simpler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
